{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(r'D:\\installed\\anaconda\\Lib\\site-packages\\en_core_web_sm\\en_core_web_sm-2.3.1')\n",
    "\n",
    "from spacy.matcher import Matcher\n",
    "m_tool = Matcher(nlp.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rule Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1 = [{'LOWER': 'quickbrownfox'}]\n",
    "p2 = [{'LOWER': 'quick'}, {'IS_PUNCT': True}, {'LOWER': 'brown'}, {'IS_PUNCT': True}, {'LOWER': 'fox'}]\n",
    "p3 = [{'LOWER': 'quick'}, {'LOWER': 'brown'}, {'LOWER': 'fox'}]\n",
    "p4 = [{'LOWER': 'quick'}, {'LOWER': 'brownfox'}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_tool.add('QBF', None, p1, p2, p3, p4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = nlp(u'The quick-brown-fox jumps over the lazy dog. The quick brown fox eats well. \\\n",
    "               the quickbrownfox is dead. the dog misses the quick brownfox')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(12825528024649263697, 1, 6), (12825528024649263697, 13, 16), (12825528024649263697, 21, 22), (12825528024649263697, 29, 31)]\n"
     ]
    }
   ],
   "source": [
    "phrase_matches = m_tool(sentence)\n",
    "print(phrase_matches )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12825528024649263697 QBF 1 6 quick-brown-fox\n",
      "12825528024649263697 QBF 13 16 quick brown fox\n",
      "12825528024649263697 QBF 21 22 quickbrownfox\n",
      "12825528024649263697 QBF 29 31 quick brownfox\n"
     ]
    }
   ],
   "source": [
    "for match_id, start, end in phrase_matches:\n",
    "    string_id = nlp.vocab.strings[match_id]  \n",
    "    span = sentence[start:end]                   \n",
    "    print(match_id, string_id, start, end, span.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse Phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4 as bs  \n",
    "import urllib.request  \n",
    "import re  \n",
    "import nltk\n",
    "\n",
    "scrapped_data = urllib.request.urlopen('https://en.wikipedia.org/wiki/Artificial_intelligence')  \n",
    "article = scrapped_data .read()\n",
    "\n",
    "parsed_article = bs.BeautifulSoup(article,'html.parser')\n",
    "\n",
    "paragraphs = parsed_article.find_all('p')\n",
    "\n",
    "article_text = \"\"\n",
    "\n",
    "for p in paragraphs:  \n",
    "    article_text += p.text\n",
    "    \n",
    "    \n",
    "processed_article = article_text.lower()  \n",
    "processed_article = re.sub('[^a-zA-Z]', ' ', processed_article )  \n",
    "processed_article = re.sub(r'\\s+', ' ', processed_article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(r'D:\\installed\\anaconda\\Lib\\site-packages\\en_core_web_sm\\en_core_web_sm-2.3.1')\n",
    "\n",
    "\n",
    "from spacy.matcher import PhraseMatcher\n",
    "phrase_matcher = PhraseMatcher(nlp.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrases = ['machine learning', 'robots', 'intelligent agents']\n",
    "\n",
    "patterns = [nlp(text) for text in phrases]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrase_matcher.add('AI', None, *patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = nlp (processed_article)\n",
    "\n",
    "matched_phrases = phrase_matcher(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5530044837203964789 AI 28 30 intelligent agents\n",
      "5530044837203964789 AI 253 255 machine learning\n",
      "5530044837203964789 AI 540 541 robots\n",
      "5530044837203964789 AI 1216 1218 machine learning\n",
      "5530044837203964789 AI 1532 1534 intelligent agents\n",
      "5530044837203964789 AI 3175 3177 intelligent agents\n",
      "5530044837203964789 AI 3334 3336 machine learning\n",
      "5530044837203964789 AI 3846 3847 robots\n",
      "5530044837203964789 AI 5294 5295 robots\n",
      "5530044837203964789 AI 5371 5372 robots\n",
      "5530044837203964789 AI 6481 6482 robots\n",
      "5530044837203964789 AI 6676 6677 robots\n",
      "5530044837203964789 AI 7536 7537 robots\n",
      "5530044837203964789 AI 8892 8893 robots\n",
      "5530044837203964789 AI 9013 9014 robots\n",
      "5530044837203964789 AI 9057 9058 robots\n"
     ]
    }
   ],
   "source": [
    "for match_id, start, end in matched_phrases:\n",
    "    string_id = nlp.vocab.strings[match_id]  \n",
    "    span = sentence[start:end]                   \n",
    "    print(match_id, string_id, start, end, span.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similariy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy \n",
    "  \n",
    "nlp = spacy.load(r'D:\\installed\\anaconda\\Lib\\site-packages\\en_core_web_lg\\en_core_web_lg-2.3.1') \n",
    "  \n",
    "words = ['Analysis on','Analysis Complete']\n",
    "  \n",
    "tokens = nlp(words) \n",
    "  \n",
    "for token in tokens: \n",
    "    # Printing the following attributes of each token. \n",
    "    # text: the word string, has_vector: if it contains \n",
    "    # a vector representation in the model,  \n",
    "    # vector_norm: the algebraic norm of the vector, \n",
    "    # is_oov: if the word is out of vocabulary. \n",
    "    print(token.text, token.has_vector, token.vector_norm, token.is_oov) \n",
    "  \n",
    "token1, token2 = tokens[0], tokens[1] \n",
    "  \n",
    "print(\"Similarity:\", token1.similarity(token2)) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
